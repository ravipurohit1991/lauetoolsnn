{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad431ffc",
   "metadata": {},
   "source": [
    "## step 1: define material and other parameters for simulating Laue patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3284dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_2000data\n",
      "Generating HKL objects\n",
      "Removing harmonics and building equivalent HKL objects\n",
      "Finalizing the HKL objects\n",
      "Saved class HKL data in : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_2000data//classhkl_data_ZrO2.pickle\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Great! No two HKL class have same angular distribution\n",
      "Generating training_data and saving them\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Great! No two HKL class have same angular distribution\n",
      "Generating testing_data and saving them\n",
      "First material index length: 287\n",
      "Class ID and frequency; check for data imbalance and select                             appropriate LOSS function for training the model\n",
      "[(235, 6117), (191, 6067), (190, 6060), (132, 6026), (236, 6019), (258, 6006), (246, 6004), (209, 6002), (163, 6000), (134, 5998), (253, 5997), (115, 5995), (219, 5991), (269, 5990), (242, 5987), (252, 5986), (208, 5986), (143, 5983), (214, 5980), (247, 5978), (241, 5975), (154, 5974), (202, 5973), (229, 5969), (105, 5966), (226, 5965), (264, 5964), (201, 5963), (228, 5963), (174, 5963), (259, 5962), (133, 5962), (257, 5961), (262, 5955), (162, 5954), (243, 5953), (87, 5951), (273, 5950), (248, 5948), (122, 5948), (227, 5946), (230, 5946), (221, 5944), (263, 5941), (121, 5941), (251, 5940), (176, 5940), (265, 5937), (104, 5936), (240, 5932), (204, 5930), (275, 5930), (215, 5929), (175, 5929), (196, 5928), (146, 5926), (205, 5924), (124, 5923), (231, 5922), (225, 5922), (123, 5922), (155, 5922), (270, 5920), (238, 5920), (180, 5920), (280, 5918), (145, 5917), (141, 5915), (213, 5915), (220, 5914), (144, 5913), (142, 5912), (274, 5912), (165, 5912), (116, 5912), (173, 5912), (198, 5911), (260, 5910), (237, 5909), (67, 5909), (276, 5908), (98, 5908), (192, 5907), (195, 5906), (164, 5903), (194, 5903), (88, 5903), (254, 5902), (89, 5901), (244, 5900), (283, 5900), (212, 5900), (261, 5899), (249, 5899), (272, 5899), (222, 5897), (68, 5897), (218, 5897), (159, 5893), (106, 5893), (279, 5892), (168, 5892), (197, 5890), (169, 5890), (250, 5889), (281, 5888), (149, 5887), (224, 5887), (150, 5884), (55, 5883), (179, 5881), (268, 5880), (199, 5874), (156, 5874), (200, 5873), (207, 5873), (167, 5873), (45, 5873), (239, 5871), (20, 5870), (138, 5869), (153, 5869), (166, 5865), (128, 5863), (19, 5861), (56, 5861), (66, 5860), (43, 5858), (137, 5858), (177, 5856), (18, 5855), (255, 5855), (282, 5853), (232, 5852), (271, 5851), (284, 5850), (152, 5848), (9, 5848), (266, 5847), (34, 5847), (151, 5844), (171, 5841), (54, 5841), (203, 5838), (99, 5835), (158, 5834), (216, 5833), (148, 5833), (32, 5833), (210, 5832), (178, 5831), (107, 5831), (206, 5830), (97, 5830), (44, 5829), (181, 5828), (110, 5828), (15, 5826), (72, 5823), (35, 5823), (140, 5822), (127, 5822), (157, 5822), (193, 5821), (46, 5821), (139, 5819), (103, 5818), (125, 5818), (170, 5816), (57, 5816), (21, 5816), (52, 5815), (95, 5813), (160, 5811), (135, 5810), (73, 5808), (74, 5808), (75, 5806), (285, 5806), (100, 5805), (65, 5805), (23, 5804), (63, 5802), (119, 5802), (31, 5800), (113, 5800), (96, 5799), (245, 5799), (101, 5797), (64, 5796), (136, 5795), (108, 5795), (33, 5794), (5, 5793), (126, 5790), (129, 5788), (118, 5788), (211, 5787), (161, 5787), (90, 5785), (114, 5784), (58, 5779), (24, 5775), (4, 5774), (172, 5774), (93, 5769), (53, 5766), (16, 5765), (109, 5764), (22, 5764), (130, 5763), (217, 5762), (182, 5761), (94, 5760), (62, 5760), (14, 5754), (61, 5753), (36, 5753), (102, 5751), (12, 5750), (50, 5749), (42, 5748), (117, 5744), (25, 5739), (277, 5738), (111, 5738), (120, 5737), (256, 5732), (13, 5732), (112, 5731), (10, 5730), (40, 5728), (69, 5727), (147, 5726), (3, 5725), (41, 5724), (47, 5719), (11, 5717), (39, 5717), (49, 5717), (70, 5715), (30, 5715), (92, 5713), (51, 5703), (91, 5702), (17, 5699), (59, 5694), (223, 5693), (71, 5692), (131, 5685), (26, 5685), (7, 5671), (48, 5671), (233, 5663), (60, 5639), (29, 5636), (28, 5635), (183, 5625), (38, 5625), (267, 5622), (37, 5612), (8, 5606), (27, 5580), (2, 5559), (286, 5515), (1, 5502), (234, 5480), (278, 5437), (184, 3069), (79, 2985), (189, 2982), (78, 2972), (185, 2969), (187, 2967), (76, 2952), (188, 2928), (84, 2922), (77, 2916), (86, 2907), (186, 2891), (83, 2886), (85, 2878), (82, 2858), (80, 2853), (81, 2842), (0, 2761), (6, 2695)]\n",
      "HKL : [-5.  1.  5.]; occurance : 6117: NN_weights : 1\n",
      "HKL : [-4.  1.  4.]; occurance : 6067: NN_weights : 1\n",
      "HKL : [-5.  1.  4.]; occurance : 6060: NN_weights : 1\n",
      "HKL : [-5.  1.  3.]; occurance : 6026: NN_weights : 1\n",
      "HKL : [-4.  1.  5.]; occurance : 6019: NN_weights : 1\n",
      "HKL : [-4.  3.  5.]; occurance : 6006: NN_weights : 1\n",
      "HKL : [-5.  2.  5.]; occurance : 6004: NN_weights : 1\n",
      "HKL : [-3.  3.  4.]; occurance : 6002: NN_weights : 1\n",
      "HKL : [-4.  4.  3.]; occurance : 6000: NN_weights : 1\n",
      "HKL : [-3.  1.  3.]; occurance : 5998: NN_weights : 1\n",
      "HKL : [2. 2. 5.]; occurance : 5997: NN_weights : 1\n",
      "HKL : [-5.  4.  2.]; occurance : 5995: NN_weights : 1\n",
      "HKL : [-3.  4.  4.]; occurance : 5991: NN_weights : 1\n",
      "HKL : [-4.  4.  5.]; occurance : 5990: NN_weights : 1\n",
      "HKL : [2. 1. 5.]; occurance : 5987: NN_weights : 1\n",
      "HKL : [1. 2. 5.]; occurance : 5986: NN_weights : 1\n",
      "HKL : [-4.  3.  4.]; occurance : 5986: NN_weights : 1\n",
      "HKL : [-5.  2.  3.]; occurance : 5983: NN_weights : 1\n",
      "HKL : [2. 3. 4.]; occurance : 5980: NN_weights : 1\n",
      "HKL : [-4.  2.  5.]; occurance : 5978: NN_weights : 1\n",
      "HKL : [1. 1. 5.]; occurance : 5975: NN_weights : 1\n",
      "HKL : [-5.  3.  3.]; occurance : 5974: NN_weights : 1\n",
      "HKL : [-3.  2.  4.]; occurance : 5973: NN_weights : 1\n",
      "HKL : [0. 5. 4.]; occurance : 5969: NN_weights : 1\n",
      "HKL : [-4.  3.  2.]; occurance : 5966: NN_weights : 1\n",
      "HKL : [-3.  5.  4.]; occurance : 5965: NN_weights : 1\n",
      "HKL : [2. 3. 5.]; occurance : 5964: NN_weights : 1\n",
      "HKL : [-5.  2.  4.]; occurance : 5963: NN_weights : 1\n",
      "HKL : [-1.  5.  4.]; occurance : 5963: NN_weights : 1\n",
      "HKL : [-4.  5.  3.]; occurance : 5963: NN_weights : 1\n",
      "HKL : [-3.  3.  5.]; occurance : 5962: NN_weights : 1\n",
      "HKL : [-4.  1.  3.]; occurance : 5962: NN_weights : 1\n",
      "HKL : [-5.  3.  5.]; occurance : 5961: NN_weights : 1\n",
      "HKL : [0. 3. 5.]; occurance : 5955: NN_weights : 1\n",
      "HKL : [-5.  4.  3.]; occurance : 5954: NN_weights : 1\n",
      "HKL : [3. 1. 5.]; occurance : 5953: NN_weights : 1\n",
      "HKL : [-5.  1.  2.]; occurance : 5951: NN_weights : 1\n",
      "HKL : [0. 4. 5.]; occurance : 5950: NN_weights : 1\n",
      "HKL : [-3.  2.  5.]; occurance : 5948: NN_weights : 1\n",
      "HKL : [-4.  5.  2.]; occurance : 5948: NN_weights : 1\n",
      "HKL : [-2.  5.  4.]; occurance : 5946: NN_weights : 1\n",
      "HKL : [1. 5. 4.]; occurance : 5946: NN_weights : 1\n",
      "HKL : [1. 4. 4.]; occurance : 5944: NN_weights : 1\n",
      "HKL : [1. 3. 5.]; occurance : 5941: NN_weights : 1\n",
      "HKL : [-5.  5.  2.]; occurance : 5941: NN_weights : 1\n",
      "HKL : [0. 2. 5.]; occurance : 5940: NN_weights : 1\n",
      "HKL : [-2.  5.  3.]; occurance : 5940: NN_weights : 1\n",
      "HKL : [3. 3. 5.]; occurance : 5937: NN_weights : 1\n",
      "HKL : [-5.  3.  2.]; occurance : 5936: NN_weights : 1\n",
      "HKL : [0. 1. 5.]; occurance : 5932: NN_weights : 1\n",
      "HKL : [1. 2. 4.]; occurance : 5930: NN_weights : 1\n",
      "HKL : [2. 4. 5.]; occurance : 5930: NN_weights : 1\n",
      "HKL : [3. 3. 4.]; occurance : 5929: NN_weights : 1\n",
      "HKL : [-3.  5.  3.]; occurance : 5929: NN_weights : 1\n",
      "HKL : [1. 1. 4.]; occurance : 5928: NN_weights : 1\n",
      "HKL : [-2.  2.  3.]; occurance : 5926: NN_weights : 1\n",
      "HKL : [3. 2. 4.]; occurance : 5924: NN_weights : 1\n",
      "HKL : [-2.  5.  2.]; occurance : 5923: NN_weights : 1\n",
      "HKL : [2. 5. 4.]; occurance : 5922: NN_weights : 1\n",
      "HKL : [-4.  5.  4.]; occurance : 5922: NN_weights : 1\n",
      "HKL : [-3.  5.  2.]; occurance : 5922: NN_weights : 1\n",
      "HKL : [-4.  3.  3.]; occurance : 5922: NN_weights : 1\n",
      "HKL : [-3.  4.  5.]; occurance : 5920: NN_weights : 1\n",
      "HKL : [-2.  1.  5.]; occurance : 5920: NN_weights : 1\n",
      "HKL : [2. 5. 3.]; occurance : 5920: NN_weights : 1\n",
      "HKL : [-3.  5.  5.]; occurance : 5918: NN_weights : 1\n",
      "HKL : [-3.  2.  3.]; occurance : 5917: NN_weights : 1\n",
      "HKL : [4. 1. 3.]; occurance : 5915: NN_weights : 1\n",
      "HKL : [1. 3. 4.]; occurance : 5915: NN_weights : 1\n",
      "HKL : [-1.  4.  4.]; occurance : 5914: NN_weights : 1\n",
      "HKL : [-4.  2.  3.]; occurance : 5913: NN_weights : 1\n",
      "HKL : [5. 1. 3.]; occurance : 5912: NN_weights : 1\n",
      "HKL : [1. 4. 5.]; occurance : 5912: NN_weights : 1\n",
      "HKL : [-2.  4.  3.]; occurance : 5912: NN_weights : 1\n",
      "HKL : [-3.  4.  2.]; occurance : 5912: NN_weights : 1\n",
      "HKL : [-5.  5.  3.]; occurance : 5912: NN_weights : 1\n",
      "HKL : [3. 1. 4.]; occurance : 5911: NN_weights : 1\n",
      "HKL : [-2.  3.  5.]; occurance : 5910: NN_weights : 1\n",
      "HKL : [-3.  1.  5.]; occurance : 5909: NN_weights : 1\n",
      "HKL : [-3.  5.  1.]; occurance : 5909: NN_weights : 1\n",
      "HKL : [3. 4. 5.]; occurance : 5908: NN_weights : 1\n",
      "HKL : [-5.  2.  2.]; occurance : 5908: NN_weights : 1\n",
      "HKL : [-3.  1.  4.]; occurance : 5907: NN_weights : 1\n",
      "HKL : [0. 1. 4.]; occurance : 5906: NN_weights : 1\n",
      "HKL : [-3.  4.  3.]; occurance : 5903: NN_weights : 1\n",
      "HKL : [-1.  1.  4.]; occurance : 5903: NN_weights : 1\n",
      "HKL : [-4.  1.  2.]; occurance : 5903: NN_weights : 1\n",
      "HKL : [3. 2. 5.]; occurance : 5902: NN_weights : 1\n",
      "HKL : [-3.  1.  2.]; occurance : 5901: NN_weights : 1\n",
      "HKL : [4. 1. 5.]; occurance : 5900: NN_weights : 1\n",
      "HKL : [1. 5. 5.]; occurance : 5900: NN_weights : 1\n",
      "HKL : [0. 3. 4.]; occurance : 5900: NN_weights : 1\n",
      "HKL : [-1.  3.  5.]; occurance : 5899: NN_weights : 1\n",
      "HKL : [-2.  2.  5.]; occurance : 5899: NN_weights : 1\n",
      "HKL : [-1.  4.  5.]; occurance : 5899: NN_weights : 1\n",
      "HKL : [3. 4. 4.]; occurance : 5897: NN_weights : 1\n",
      "HKL : [-2.  5.  1.]; occurance : 5897: NN_weights : 1\n",
      "HKL : [-5.  4.  4.]; occurance : 5897: NN_weights : 1\n",
      "HKL : [2. 3. 3.]; occurance : 5893: NN_weights : 1\n",
      "HKL : [-3.  3.  2.]; occurance : 5893: NN_weights : 1\n",
      "HKL : [-4.  5.  5.]; occurance : 5892: NN_weights : 1\n",
      "HKL : [1. 4. 3.]; occurance : 5892: NN_weights : 1\n",
      "HKL : [2. 1. 4.]; occurance : 5890: NN_weights : 1\n",
      "HKL : [2. 4. 3.]; occurance : 5890: NN_weights : 1\n",
      "HKL : [-1.  2.  5.]; occurance : 5889: NN_weights : 1\n",
      "HKL : [-2.  5.  5.]; occurance : 5888: NN_weights : 1\n",
      "HKL : [1. 2. 3.]; occurance : 5887: NN_weights : 1\n",
      "HKL : [-5.  5.  4.]; occurance : 5887: NN_weights : 1\n",
      "HKL : [2. 2. 3.]; occurance : 5884: NN_weights : 1\n",
      "HKL : [-4.  4.  1.]; occurance : 5883: NN_weights : 1\n",
      "HKL : [1. 5. 3.]; occurance : 5881: NN_weights : 1\n",
      "HKL : [-5.  4.  5.]; occurance : 5880: NN_weights : 1\n",
      "HKL : [4. 1. 4.]; occurance : 5874: NN_weights : 1\n",
      "HKL : [-2.  3.  3.]; occurance : 5874: NN_weights : 1\n",
      "HKL : [5. 1. 4.]; occurance : 5873: NN_weights : 1\n",
      "HKL : [-5.  3.  4.]; occurance : 5873: NN_weights : 1\n",
      "HKL : [0. 4. 3.]; occurance : 5873: NN_weights : 1\n",
      "HKL : [-3.  3.  1.]; occurance : 5873: NN_weights : 1\n",
      "HKL : [-1.  1.  5.]; occurance : 5871: NN_weights : 1\n",
      "HKL : [4. 5. 0.]; occurance : 5870: NN_weights : 1\n",
      "HKL : [1. 1. 3.]; occurance : 5869: NN_weights : 1\n",
      "HKL : [5. 2. 3.]; occurance : 5869: NN_weights : 1\n",
      "HKL : [-1.  4.  3.]; occurance : 5865: NN_weights : 1\n",
      "HKL : [2. 5. 2.]; occurance : 5863: NN_weights : 1\n",
      "HKL : [3. 5. 0.]; occurance : 5861: NN_weights : 1\n",
      "HKL : [-3.  4.  1.]; occurance : 5861: NN_weights : 1\n",
      "HKL : [-4.  5.  1.]; occurance : 5860: NN_weights : 1\n",
      "HKL : [-5.  3.  1.]; occurance : 5858: NN_weights : 1\n",
      "HKL : [0. 1. 3.]; occurance : 5858: NN_weights : 1\n",
      "HKL : [-1.  5.  3.]; occurance : 5856: NN_weights : 1\n",
      "HKL : [2. 5. 0.]; occurance : 5855: NN_weights : 1\n",
      "HKL : [4. 2. 5.]; occurance : 5855: NN_weights : 1\n",
      "HKL : [-1.  5.  5.]; occurance : 5853: NN_weights : 1\n",
      "HKL : [3. 5. 4.]; occurance : 5852: NN_weights : 1\n",
      "HKL : [-2.  4.  5.]; occurance : 5851: NN_weights : 1\n",
      "HKL : [2. 5. 5.]; occurance : 5850: NN_weights : 1\n",
      "HKL : [4. 2. 3.]; occurance : 5848: NN_weights : 1\n",
      "HKL : [5. 2. 0.]; occurance : 5848: NN_weights : 1\n",
      "HKL : [4. 3. 5.]; occurance : 5847: NN_weights : 1\n",
      "HKL : [-3.  2.  1.]; occurance : 5847: NN_weights : 1\n",
      "HKL : [3. 2. 3.]; occurance : 5844: NN_weights : 1\n",
      "HKL : [4. 4. 3.]; occurance : 5841: NN_weights : 1\n",
      "HKL : [-5.  4.  1.]; occurance : 5841: NN_weights : 1\n",
      "HKL : [-1.  2.  4.]; occurance : 5838: NN_weights : 1\n",
      "HKL : [-3.  2.  2.]; occurance : 5835: NN_weights : 1\n",
      "HKL : [1. 3. 3.]; occurance : 5834: NN_weights : 1\n",
      "HKL : [4. 3. 4.]; occurance : 5833: NN_weights : 1\n",
      "HKL : [0. 2. 3.]; occurance : 5833: NN_weights : 1\n",
      "HKL : [-5.  2.  1.]; occurance : 5833: NN_weights : 1\n",
      "HKL : [-2.  3.  4.]; occurance : 5832: NN_weights : 1\n",
      "HKL : [0. 5. 3.]; occurance : 5831: NN_weights : 1\n",
      "HKL : [-2.  3.  2.]; occurance : 5831: NN_weights : 1\n",
      "HKL : [5. 2. 4.]; occurance : 5830: NN_weights : 1\n",
      "HKL : [5. 1. 2.]; occurance : 5830: NN_weights : 1\n",
      "HKL : [-4.  3.  1.]; occurance : 5829: NN_weights : 1\n",
      "HKL : [3. 5. 3.]; occurance : 5828: NN_weights : 1\n",
      "HKL : [1. 3. 2.]; occurance : 5828: NN_weights : 1\n",
      "HKL : [3. 4. 0.]; occurance : 5826: NN_weights : 1\n",
      "HKL : [2. 5. 1.]; occurance : 5823: NN_weights : 1\n",
      "HKL : [-2.  2.  1.]; occurance : 5823: NN_weights : 1\n",
      "HKL : [3. 1. 3.]; occurance : 5822: NN_weights : 1\n",
      "HKL : [1. 5. 2.]; occurance : 5822: NN_weights : 1\n",
      "HKL : [-1.  3.  3.]; occurance : 5822: NN_weights : 1\n",
      "HKL : [-2.  1.  4.]; occurance : 5821: NN_weights : 1\n",
      "HKL : [-2.  3.  1.]; occurance : 5821: NN_weights : 1\n",
      "HKL : [2. 1. 3.]; occurance : 5819: NN_weights : 1\n",
      "HKL : [5. 2. 2.]; occurance : 5818: NN_weights : 1\n",
      "HKL : [-1.  5.  2.]; occurance : 5818: NN_weights : 1\n",
      "HKL : [3. 4. 3.]; occurance : 5816: NN_weights : 1\n",
      "HKL : [-2.  4.  1.]; occurance : 5816: NN_weights : 1\n",
      "HKL : [-5.  1.  1.]; occurance : 5816: NN_weights : 1\n",
      "HKL : [4. 3. 1.]; occurance : 5815: NN_weights : 1\n",
      "HKL : [3. 1. 2.]; occurance : 5813: NN_weights : 1\n",
      "HKL : [4. 3. 3.]; occurance : 5811: NN_weights : 1\n",
      "HKL : [-2.  1.  3.]; occurance : 5810: NN_weights : 1\n",
      "HKL : [3. 5. 1.]; occurance : 5808: NN_weights : 1\n",
      "HKL : [4. 5. 1.]; occurance : 5808: NN_weights : 1\n",
      "HKL : [5. 5. 1.]; occurance : 5806: NN_weights : 1\n",
      "HKL : [3. 5. 5.]; occurance : 5806: NN_weights : 1\n",
      "HKL : [-1.  2.  2.]; occurance : 5805: NN_weights : 1\n",
      "HKL : [-5.  5.  1.]; occurance : 5805: NN_weights : 1\n",
      "HKL : [-3.  1.  1.]; occurance : 5804: NN_weights : 1\n",
      "HKL : [4. 4. 1.]; occurance : 5802: NN_weights : 1\n",
      "HKL : [3. 4. 2.]; occurance : 5802: NN_weights : 1\n",
      "HKL : [5. 1. 1.]; occurance : 5800: NN_weights : 1\n",
      "HKL : [4. 3. 2.]; occurance : 5800: NN_weights : 1\n",
      "HKL : [4. 1. 2.]; occurance : 5799: NN_weights : 1\n",
      "HKL : [5. 1. 5.]; occurance : 5799: NN_weights : 1\n",
      "HKL : [1. 2. 2.]; occurance : 5797: NN_weights : 1\n",
      "HKL : [5. 4. 1.]; occurance : 5796: NN_weights : 1\n",
      "HKL : [-1.  1.  3.]; occurance : 5795: NN_weights : 1\n",
      "HKL : [-1.  3.  2.]; occurance : 5795: NN_weights : 1\n",
      "HKL : [-4.  2.  1.]; occurance : 5794: NN_weights : 1\n",
      "HKL : [5. 1. 0.]; occurance : 5793: NN_weights : 1\n",
      "HKL : [0. 5. 2.]; occurance : 5790: NN_weights : 1\n",
      "HKL : [3. 5. 2.]; occurance : 5788: NN_weights : 1\n",
      "HKL : [1. 4. 2.]; occurance : 5788: NN_weights : 1\n",
      "HKL : [-1.  3.  4.]; occurance : 5787: NN_weights : 1\n",
      "HKL : [5. 3. 3.]; occurance : 5787: NN_weights : 1\n",
      "HKL : [-2.  1.  2.]; occurance : 5785: NN_weights : 1\n",
      "HKL : [5. 3. 2.]; occurance : 5784: NN_weights : 1\n",
      "HKL : [-1.  4.  1.]; occurance : 5779: NN_weights : 1\n",
      "HKL : [-2.  1.  1.]; occurance : 5775: NN_weights : 1\n",
      "HKL : [4. 1. 0.]; occurance : 5774: NN_weights : 1\n",
      "HKL : [5. 4. 3.]; occurance : 5774: NN_weights : 1\n",
      "HKL : [1. 1. 2.]; occurance : 5769: NN_weights : 1\n",
      "HKL : [5. 3. 1.]; occurance : 5766: NN_weights : 1\n",
      "HKL : [5. 4. 0.]; occurance : 5765: NN_weights : 1\n",
      "HKL : [0. 3. 2.]; occurance : 5764: NN_weights : 1\n",
      "HKL : [-4.  1.  1.]; occurance : 5764: NN_weights : 1\n",
      "HKL : [4. 5. 2.]; occurance : 5763: NN_weights : 1\n",
      "HKL : [5. 3. 4.]; occurance : 5762: NN_weights : 1\n",
      "HKL : [4. 5. 3.]; occurance : 5761: NN_weights : 1\n",
      "HKL : [2. 1. 2.]; occurance : 5760: NN_weights : 1\n",
      "HKL : [3. 4. 1.]; occurance : 5760: NN_weights : 1\n",
      "HKL : [1. 4. 0.]; occurance : 5754: NN_weights : 1\n",
      "HKL : [2. 4. 1.]; occurance : 5753: NN_weights : 1\n",
      "HKL : [-1.  2.  1.]; occurance : 5753: NN_weights : 1\n",
      "HKL : [3. 2. 2.]; occurance : 5751: NN_weights : 1\n",
      "HKL : [4. 3. 0.]; occurance : 5750: NN_weights : 1\n",
      "HKL : [2. 3. 1.]; occurance : 5749: NN_weights : 1\n",
      "HKL : [5. 2. 1.]; occurance : 5748: NN_weights : 1\n",
      "HKL : [-1.  4.  2.]; occurance : 5744: NN_weights : 1\n",
      "HKL : [-1.  1.  1.]; occurance : 5739: NN_weights : 1\n",
      "HKL : [4. 4. 5.]; occurance : 5738: NN_weights : 1\n",
      "HKL : [2. 3. 2.]; occurance : 5738: NN_weights : 1\n",
      "HKL : [5. 4. 2.]; occurance : 5737: NN_weights : 1\n",
      "HKL : [5. 2. 5.]; occurance : 5732: NN_weights : 1\n",
      "HKL : [5. 3. 0.]; occurance : 5732: NN_weights : 1\n",
      "HKL : [3. 3. 2.]; occurance : 5731: NN_weights : 1\n",
      "HKL : [1. 3. 0.]; occurance : 5730: NN_weights : 1\n",
      "HKL : [3. 2. 1.]; occurance : 5728: NN_weights : 1\n",
      "HKL : [-1.  5.  1.]; occurance : 5727: NN_weights : 1\n",
      "HKL : [-1.  2.  3.]; occurance : 5726: NN_weights : 1\n",
      "HKL : [3. 1. 0.]; occurance : 5725: NN_weights : 1\n",
      "HKL : [4. 2. 1.]; occurance : 5724: NN_weights : 1\n",
      "HKL : [-1.  3.  1.]; occurance : 5719: NN_weights : 1\n",
      "HKL : [2. 3. 0.]; occurance : 5717: NN_weights : 1\n",
      "HKL : [2. 2. 1.]; occurance : 5717: NN_weights : 1\n",
      "HKL : [1. 3. 1.]; occurance : 5717: NN_weights : 1\n",
      "HKL : [0. 5. 1.]; occurance : 5715: NN_weights : 1\n",
      "HKL : [4. 1. 1.]; occurance : 5715: NN_weights : 1\n",
      "HKL : [0. 1. 2.]; occurance : 5713: NN_weights : 1\n",
      "HKL : [3. 3. 1.]; occurance : 5703: NN_weights : 1\n",
      "HKL : [-1.  1.  2.]; occurance : 5702: NN_weights : 1\n",
      "HKL : [1. 5. 0.]; occurance : 5699: NN_weights : 1\n",
      "HKL : [0. 4. 1.]; occurance : 5694: NN_weights : 1\n",
      "HKL : [5. 4. 4.]; occurance : 5693: NN_weights : 1\n",
      "HKL : [1. 5. 1.]; occurance : 5692: NN_weights : 1\n",
      "HKL : [5. 5. 2.]; occurance : 5685: NN_weights : 1\n",
      "HKL : [0. 1. 1.]; occurance : 5685: NN_weights : 1\n",
      "HKL : [1. 2. 0.]; occurance : 5671: NN_weights : 1\n",
      "HKL : [0. 3. 1.]; occurance : 5671: NN_weights : 1\n",
      "HKL : [4. 5. 4.]; occurance : 5663: NN_weights : 1\n",
      "HKL : [1. 4. 1.]; occurance : 5639: NN_weights : 1\n",
      "HKL : [3. 1. 1.]; occurance : 5636: NN_weights : 1\n",
      "HKL : [2. 1. 1.]; occurance : 5635: NN_weights : 1\n",
      "HKL : [5. 5. 3.]; occurance : 5625: NN_weights : 1\n",
      "HKL : [1. 2. 1.]; occurance : 5625: NN_weights : 1\n",
      "HKL : [5. 3. 5.]; occurance : 5622: NN_weights : 1\n",
      "HKL : [0. 2. 1.]; occurance : 5612: NN_weights : 1\n",
      "HKL : [3. 2. 0.]; occurance : 5606: NN_weights : 1\n",
      "HKL : [1. 1. 1.]; occurance : 5580: NN_weights : 1\n",
      "HKL : [2. 1. 0.]; occurance : 5559: NN_weights : 1\n",
      "HKL : [4. 5. 5.]; occurance : 5515: NN_weights : 1\n",
      "HKL : [1. 1. 0.]; occurance : 5502: NN_weights : 1\n",
      "HKL : [5. 5. 4.]; occurance : 5480: NN_weights : 1\n",
      "HKL : [5. 4. 5.]; occurance : 5437: NN_weights : 1\n",
      "HKL : [-5.  0.  4.]; occurance : 3069: NN_weights : 1\n",
      "HKL : [-1.  0.  1.]; occurance : 2985: NN_weights : 2\n",
      "HKL : [5. 0. 4.]; occurance : 2982: NN_weights : 2\n",
      "HKL : [-3.  0.  2.]; occurance : 2972: NN_weights : 2\n",
      "HKL : [-3.  0.  4.]; occurance : 2969: NN_weights : 2\n",
      "HKL : [1. 0. 4.]; occurance : 2967: NN_weights : 2\n",
      "HKL : [-5.  0.  2.]; occurance : 2952: NN_weights : 2\n",
      "HKL : [3. 0. 4.]; occurance : 2928: NN_weights : 2\n",
      "HKL : [3. 0. 2.]; occurance : 2922: NN_weights : 2\n",
      "HKL : [-2.  0.  1.]; occurance : 2916: NN_weights : 2\n",
      "HKL : [5. 0. 2.]; occurance : 2907: NN_weights : 2\n",
      "HKL : [-1.  0.  4.]; occurance : 2891: NN_weights : 2\n",
      "HKL : [1. 0. 1.]; occurance : 2886: NN_weights : 2\n",
      "HKL : [2. 0. 1.]; occurance : 2878: NN_weights : 2\n",
      "HKL : [1. 0. 2.]; occurance : 2858: NN_weights : 2\n",
      "HKL : [-1.  0.  2.]; occurance : 2853: NN_weights : 2\n",
      "HKL : [0. 0. 1.]; occurance : 2842: NN_weights : 2\n",
      "HKL : [1. 0. 0.]; occurance : 2761: NN_weights : 2\n",
      "HKL : [0. 1. 0.]; occurance : 2695: NN_weights : 2\n",
      "0 classes removed from the classHKL object [removal frequency: 1] (before:287, now:287)\n",
      "0 classes removed from the classHKL object [removal frequency: 1] (before:287, now:287)\n",
      "Saved class weights data\n",
      "900 287\n",
      "Number of spots in a batch of 50 files : 7968\n",
      "Min, Max class ID is 0, 286\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 989s 5s/step - loss: 2.6886 - fn: 1065235.0000 - fp: 43914.0000 - tn: 463523424.0000 - tp: 555630.0000 - precision: 0.9268 - accuracy: 0.3428 - val_loss: 1.0049 - val_fn: 94325.0000 - val_fp: 8027.0000 - val_tn: 93128160.0000 - val_tp: 231326.0000 - val_precision: 0.9665 - val_accuracy: 0.7103\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 998s 5s/step - loss: 1.2529 - fn: 556888.0000 - fp: 74913.0000 - tn: 463492480.0000 - tp: 1063977.0000 - precision: 0.9342 - accuracy: 0.6564 - val_loss: 0.8317 - val_fn: 73661.0000 - val_fp: 8457.0000 - val_tn: 93127728.0000 - val_tp: 251990.0000 - val_precision: 0.9675 - val_accuracy: 0.7738\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 1011s 5s/step - loss: 1.0844 - fn: 470239.0000 - fp: 74949.0000 - tn: 463492480.0000 - tp: 1150626.0000 - precision: 0.9388 - accuracy: 0.7099 - val_loss: 0.7783 - val_fn: 67453.0000 - val_fp: 8456.0000 - val_tn: 93127728.0000 - val_tp: 258198.0000 - val_precision: 0.9683 - val_accuracy: 0.7929\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 1016s 5s/step - loss: 1.0050 - fn: 429298.0000 - fp: 74331.0000 - tn: 463492928.0000 - tp: 1191567.0000 - precision: 0.9413 - accuracy: 0.7351 - val_loss: 0.7408 - val_fn: 63260.0000 - val_fp: 8031.0000 - val_tn: 93128128.0000 - val_tp: 262391.0000 - val_precision: 0.9703 - val_accuracy: 0.8057\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 1011s 5s/step - loss: 0.9491 - fn: 401792.0000 - fp: 73315.0000 - tn: 463494016.0000 - tp: 1219073.0000 - precision: 0.9433 - accuracy: 0.7521 - val_loss: 0.7244 - val_fn: 60753.0000 - val_fp: 8298.0000 - val_tn: 93127904.0000 - val_tp: 264898.0000 - val_precision: 0.9696 - val_accuracy: 0.8134\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 1014s 5s/step - loss: 0.9101 - fn: 382436.0000 - fp: 71698.0000 - tn: 463495840.0000 - tp: 1238429.0000 - precision: 0.9453 - accuracy: 0.7641 - val_loss: 0.7075 - val_fn: 58798.0000 - val_fp: 8177.0000 - val_tn: 93127992.0000 - val_tp: 266853.0000 - val_precision: 0.9703 - val_accuracy: 0.8194\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 1010s 5s/step - loss: 0.8775 - fn: 367057.0000 - fp: 71232.0000 - tn: 463496224.0000 - tp: 1253808.0000 - precision: 0.9462 - accuracy: 0.7735 - val_loss: 0.7008 - val_fn: 57822.0000 - val_fp: 7963.0000 - val_tn: 93128224.0000 - val_tp: 267829.0000 - val_precision: 0.9711 - val_accuracy: 0.8224\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 1014s 5s/step - loss: 0.8500 - fn: 353005.0000 - fp: 70371.0000 - tn: 463497056.0000 - tp: 1267860.0000 - precision: 0.9474 - accuracy: 0.7822 - val_loss: 0.6846 - val_fn: 55350.0000 - val_fp: 8125.0000 - val_tn: 93128080.0000 - val_tp: 270301.0000 - val_precision: 0.9708 - val_accuracy: 0.8300\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 1019s 5s/step - loss: 0.8274 - fn: 340852.0000 - fp: 69408.0000 - tn: 463498112.0000 - tp: 1280013.0000 - precision: 0.9486 - accuracy: 0.7897 - val_loss: 0.6786 - val_fn: 54560.0000 - val_fp: 7711.0000 - val_tn: 93128472.0000 - val_tp: 271091.0000 - val_precision: 0.9723 - val_accuracy: 0.8325\n",
      "Epoch 10/20\n",
      "  4/200 [..............................] - ETA: 15:26 - loss: 0.8558 - fn: 6928.0000 - fp: 1350.0000 - tn: 9046546.0000 - tp: 24708.0000 - precision: 0.9482 - accuracy: 0.7810"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12428\\3276942499.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;31m## Fitting function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         stats_model = model.fit(\n\u001b[0m\u001b[0;32m    303\u001b[0m                                 \u001b[0mtraining_data_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\laueNN\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "\n",
    "    ## if LaueToolsNN is properly installed\n",
    "    try:\n",
    "        from lauetoolsnn.utils_lauenn import generate_classHKL, generate_dataset, rmv_freq_class, get_material_detail\n",
    "    except:\n",
    "        # else import from a path where LaueToolsNN files are\n",
    "        import sys\n",
    "        sys.path.append(r\"USER_PATH_HERE\")\n",
    "        from utils_lauenn import generate_classHKL, generate_dataset, rmv_freq_class,  get_material_detail\n",
    "        \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import _pickle as cPickle\n",
    "    import itertools\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ## if LaueToolsNN is properly installed\n",
    "    try:\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "    except:\n",
    "        # else import from a path where LaueToolsNN files are\n",
    "        import sys\n",
    "        sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "        from utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from tqdm import trange\n",
    "\n",
    "    ## if LaueToolsNN is properly installed\n",
    "    try:\n",
    "        from lauetoolsnn.utils_lauenn import get_material_detail, prepare_LP_NB\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.lauetools import IOLaueTools as IOLT\n",
    "    except:\n",
    "        # else import from a path where LaueToolsNN files are\n",
    "        import sys\n",
    "        sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "        from utils_lauenn import get_material_detail, prepare_LP_NB\n",
    "        sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\lauetools\")\n",
    "        import dict_LaueTools as dictLT\n",
    "        import IOLaueTools as IOLT\n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    # =============================================================================\n",
    "    \n",
    "    text_file_logger = open(\"logger_ZrO2.txt\", \"w\")\n",
    "    bss = [1,5,10,15,20,20,20,20,20,50,50,50,50,50,50,50,50,50]\n",
    "    nmbr = [10,50,100,150,200,250,300,350,400,450,500,550,600,700,800,900,1000,2000]\n",
    "    for inums in range(len(nmbr)):\n",
    "        \n",
    "        epochs = 20\n",
    "        batch_size = bss[inums]\n",
    "        inum = nmbr[inums]\n",
    "        if inum != 2000:\n",
    "            continue\n",
    "        \n",
    "        input_params = {\n",
    "                        \"material_\": \"ZrO2\",             ## same key as used in dict_LaueTools\n",
    "                        \"material1_\": \"ZrO2\",            ## same key as used in dict_LaueTools\n",
    "                        \"prefix\" : \"data\",                 ## prefix for the folder to be created for training dataset\n",
    "                        \"symmetry\": \"monoclinic\",           ## crystal symmetry of material_\n",
    "                        \"symmetry1\": \"monoclinic\",          ## crystal symmetry of material1_\n",
    "                        \"SG\": 14,                     ## Space group of material_ (None if not known)\n",
    "                        \"SG1\": 14,                    ## Space group of material1_ (None if not known)\n",
    "                        \"hkl_max_identify\" : 5,        ## Maximum hkl index to classify in a Laue pattern\n",
    "                        \"maximum_angle_to_search\":90,  ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                        \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                        \"nb_grains_per_lp\" : 5,        ## max grains to be generated in a Laue Image\n",
    "                        \"grains_nb_simulate\" : inum,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                        ## Detector parameters (roughly) of the Experimental setup\n",
    "                        ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                        \"detectorparameters\" :  [79.553,979.32,932.31,0.37,0.447], \n",
    "                        \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                        \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                        \"dim2\":2016,\n",
    "                        \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                        \"emax\" : 22,\n",
    "                        }\n",
    "\n",
    "        input_params[\"prefix\"] = \"_\" + str(input_params[\"grains_nb_simulate\"]) + input_params[\"prefix\"]\n",
    "        text_file_logger.write(input_params[\"prefix\"] + \"\\n\")\n",
    "        material_= input_params[\"material_\"]\n",
    "        material1_= input_params[\"material1_\"]\n",
    "        n = input_params[\"hkl_max_identify\"]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        symm1_ = input_params[\"symmetry1\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        SG1 = input_params[\"SG1\"]\n",
    "\n",
    "        if material_ != material1_:\n",
    "            save_directory = os.getcwd()+\"//\"+material_+\"_\"+material1_+input_params[\"prefix\"]\n",
    "        else:\n",
    "            save_directory = os.getcwd()+\"//\"+material_+input_params[\"prefix\"]\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "            crystal, SG, rules1, symmetry1,\\\n",
    "            lattice_material1, crystal1, SG1 = get_material_detail(material_, SG, symm_,\n",
    "                                                                   material1_, SG1, symm1_)\n",
    "\n",
    "        ## procedure for generation of GROUND TRUTH classes\n",
    "        # general_diff_cond = True will eliminate the hkl index that does not satisfy the general reflection conditions\n",
    "        generate_classHKL(n, rules, lattice_material, symmetry, material_, crystal=crystal, SG=SG, general_diff_cond=True,\n",
    "                  save_directory=save_directory, write_to_console=print, ang_maxx = maximum_angle_to_search, \n",
    "                  step = step_for_binning)\n",
    "\n",
    "        if material_ != material1_:\n",
    "            generate_classHKL(n, rules1, lattice_material1, symmetry1, material1_, crystal=crystal1, SG=SG1, general_diff_cond=True,\n",
    "                      save_directory=save_directory, write_to_console=print, ang_maxx = maximum_angle_to_search, \n",
    "                      step = step_for_binning)\n",
    "        ############ GENERATING TRAINING DATA ##############\n",
    "        # data_realism =True ; will introduce noise and partial Laue patterns in the training dataset\n",
    "        # modelp can have either \"random\" for random orientation generation or \"uniform\" for uniform orientation generation\n",
    "        # include_scm (if True; misorientation_angle parameter need to be defined): this parameter introduces misoriented crystal of specific angle along a crystal axis in the training dataset\n",
    "        generate_dataset(material_=material_, material1_=material1_, ang_maxx=maximum_angle_to_search,\n",
    "                             step=step_for_binning, mode=0, \n",
    "                             nb_grains=nb_grains_per_lp, nb_grains1=nb_grains_per_lp, \n",
    "                             grains_nb_simulate=grains_nb_simulate, data_realism = True, \n",
    "                             detectorparameters=detectorparameters, pixelsize=pixelsize, type_=\"training_data\",\n",
    "                             var0 = 1, dim1=input_params[\"dim1\"], dim2=input_params[\"dim2\"], \n",
    "                             removeharmonics=1, save_directory=save_directory,\n",
    "                            write_to_console=print, emin=emin, emax=emax, modelp = \"random\",\n",
    "                            misorientation_angle = 1, general_diff_rules = False, \n",
    "                            crystal = crystal, crystal1 = crystal1, include_scm=False,)\n",
    "\n",
    "        ############ GENERATING TESTING DATA ##############\n",
    "        factor = 5 # validation split for the training dataset  --> corresponds to 20% of total training dataset\n",
    "        generate_dataset(material_=material_, material1_=material1_, ang_maxx=maximum_angle_to_search,\n",
    "                             step=step_for_binning, mode=0, \n",
    "                             nb_grains=nb_grains_per_lp, nb_grains1=nb_grains_per_lp, \n",
    "                             grains_nb_simulate=grains_nb_simulate//factor, data_realism = True, \n",
    "                             detectorparameters=detectorparameters, pixelsize=pixelsize, type_=\"testing_data\",\n",
    "                             var0 = 1, dim1=input_params[\"dim1\"], dim2=input_params[\"dim2\"], \n",
    "                             removeharmonics=1, save_directory=save_directory,\n",
    "                            write_to_console=print, emin=emin, emax=emax, modelp = \"random\",\n",
    "                            misorientation_angle = 1, general_diff_rules = False, \n",
    "                            crystal = crystal, crystal1 = crystal1, include_scm=False,)\n",
    "\n",
    "        ## Updating the ClassHKL list by removing the non-common HKL or less frequent HKL from the list\n",
    "        ## The non-common HKL can occur as a result of the detector position and energy used\n",
    "        # freq_rmv: remove output hkl if the training dataset has less tha 100 occurances of the considered hkl (freq_rmv1 for second phase)\n",
    "        # Weights (penalty during training) are also calculated based on the occurance\n",
    "        rmv_freq_class(freq_rmv = 1, freq_rmv1 = 1,\n",
    "                            save_directory=save_directory, material_=material_, \n",
    "                            material1_=material1_, write_to_console=print)\n",
    "\n",
    "        ## End of data generation for Neural network training: all files are saved in the same folder to be later used for training and prediction\n",
    "        import numpy as np\n",
    "        import _pickle as cPickle\n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "        with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "            class_weights = cPickle.load(input_file)\n",
    "        class_weights = class_weights[0]\n",
    "\n",
    "        n_bins = len(angbins)-1\n",
    "        n_outputs = len(classhkl)\n",
    "        print(n_bins, n_outputs)\n",
    "\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.layers import BatchNormalization\n",
    "        import keras\n",
    "        from keras.regularizers import l2\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense, Activation, Dropout\n",
    "        from keras.constraints import maxnorm\n",
    "\n",
    "        metricsNN = [\n",
    "                    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "                    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "                    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "                    keras.metrics.TruePositives(name=\"tp\"),\n",
    "                    keras.metrics.Precision(name=\"precision\"),\n",
    "                    keras.metrics.Recall(name=\"accuracy\"),\n",
    "                    ]\n",
    "\n",
    "        def model_arch_general_optimized(n_bins, n_outputs, kernel_coeff = 0.0005, bias_coeff = 0.0005, lr=None, verbose=0,\n",
    "                               write_to_console=None):\n",
    "            \"\"\"\n",
    "            Very simple and straight forward Neural Network with few hyperparameters\n",
    "            straighforward RELU activation strategy with cross entropy to identify the HKL\n",
    "            Tried BatchNormalization --> no significant impact\n",
    "            Tried weighted approach --> not better for HCP\n",
    "            Trying Regularaization \n",
    "            l2(0.001) means that every coefficient in the weight matrix of the layer \n",
    "            will add 0.001 * weight_coefficient_value**2 to the total loss of the network\n",
    "            1e-3,1e-5,1e-6\n",
    "            \"\"\"\n",
    "            if n_outputs >= n_bins:\n",
    "                param = n_bins\n",
    "                if param*15 < (2*n_outputs): ## quick hack; make Proper implementation\n",
    "                    param = (n_bins + n_outputs)//2\n",
    "            else:\n",
    "                # param = n_outputs ## More reasonable ???\n",
    "                param = n_outputs*2 ## More reasonable ???\n",
    "                # param = n_bins//2\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(keras.Input(shape=(n_bins,)))\n",
    "            ## Hidden layer 1\n",
    "            model.add(Dense(n_bins, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "            # model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dropout(0.3)) ## Adding dropout as we introduce some uncertain data with noise\n",
    "            ## Hidden layer 2\n",
    "            model.add(Dense(((param)*15 + n_bins)//2, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "            # model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dropout(0.3))\n",
    "            ## Hidden layer 3\n",
    "            model.add(Dense((param)*15, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "            # model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dropout(0.3))\n",
    "            ## Output layer \n",
    "            model.add(Dense(n_outputs, activation='softmax'))\n",
    "            ## Compile model\n",
    "            if lr != None:\n",
    "                otp = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=otp, metrics=[metricsNN])\n",
    "            else:\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[metricsNN])\n",
    "\n",
    "            if verbose == 1:\n",
    "                model.summary()\n",
    "                stringlist = []\n",
    "                model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "                short_model_summary = \"\\n\".join(stringlist)\n",
    "                if write_to_console!=None:\n",
    "                    write_to_console(short_model_summary)\n",
    "            return model\n",
    "\n",
    "        # load model and train\n",
    "        #neurons_multiplier is a list with number of neurons per layer, the first value is input shape and last value is output shape, inbetween are the number of neurons per hidden layers\n",
    "        model = model_arch_general_optimized(  n_bins, n_outputs,\n",
    "                                               kernel_coeff = 1e-5,\n",
    "                                               bias_coeff = 1e-6,\n",
    "                                               lr = 1e-3,\n",
    "                                                )\n",
    "        #model = model_arch_general_optimized(n_bins, n_outputs, kernel_coeff = 0.0005, bias_coeff = 0.0005, lr=None, verbose=1,)\n",
    "        ## temp function to quantify the spots and classes present in a batch\n",
    "        \n",
    "        trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", batch_size, \n",
    "                                                len(classhkl), loc_new, print)\n",
    "        print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "        print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "\n",
    "        ## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "        nb_grains_per_lp1 = nb_grains_per_lp\n",
    "        if material_ != material1_:\n",
    "            nb_grains_list = list(range(nb_grains_per_lp+1))\n",
    "            nb_grains1_list = list(range(nb_grains_per_lp1+1))\n",
    "            list_permute = list(itertools.product(nb_grains_list, nb_grains1_list))\n",
    "            list_permute.pop(0)\n",
    "            steps_per_epoch = (len(list_permute) * grains_nb_simulate)//batch_size\n",
    "        else:\n",
    "            steps_per_epoch = int((nb_grains_per_lp * grains_nb_simulate) / batch_size)\n",
    "\n",
    "        val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "        if steps_per_epoch == 0:\n",
    "            steps_per_epoch = 1\n",
    "        if val_steps_per_epoch == 0:\n",
    "            val_steps_per_epoch = 1 \n",
    "\n",
    "        ## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "        training_data_generator = array_generator(save_directory+\"//training_data\", batch_size, \\\n",
    "                                                  len(classhkl), loc_new, print)\n",
    "        testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size, \\\n",
    "                                                  len(classhkl), loc_new, print)\n",
    "\n",
    "        ######### TRAIN THE DATA\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "        ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", monitor='val_accuracy', \n",
    "                              mode='max', save_best_only=True)\n",
    "\n",
    "        # model save directory and filename\n",
    "        if material_ != material1_:\n",
    "            model_name = save_directory+\"//model_\"+material_+\"_\"+material1_\n",
    "        else:\n",
    "            model_name = save_directory+\"//model_\"+material_\n",
    "\n",
    "        ## Fitting function\n",
    "        stats_model = model.fit(\n",
    "                                training_data_generator, \n",
    "                                epochs=epochs, \n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=testing_data_generator,\n",
    "                                validation_steps=val_steps_per_epoch,\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights,\n",
    "                                callbacks=[es, ms]\n",
    "                                )\n",
    "\n",
    "        # Save model config and weights\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)            \n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "        print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "        print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "        print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "        print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "\n",
    "        # Plot the accuracy/loss v Epochs\n",
    "        epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "        ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "        ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "        if material_ != material1_:\n",
    "            plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\"_\"+material1_+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "        plt.close()\n",
    "\n",
    "        if material_ != material1_:\n",
    "            text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "        else:\n",
    "            text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\".txt\", \"w\")\n",
    "\n",
    "        text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "        for inj in range(len(epochs)):\n",
    "            string1 = str(epochs[inj]) + \",\"+ str(model.history.history['loss'][inj])+\\\n",
    "                    \",\"+str(model.history.history['val_loss'][inj])+\",\"+str(model.history.history['accuracy'][inj])+\\\n",
    "                    \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "            text_file.write(string1)\n",
    "        text_file.close() \n",
    "\n",
    "        from lattice import HklPlane\n",
    "        \n",
    "\n",
    "        def commonclass(hkl1, hkl2, lattice_material, symmetry_):\n",
    "            \"\"\" test if hkl1 and hkl2 belong to the same class\"\"\"\n",
    "            h,k,l = hkl1\n",
    "            h1,k1,l1 = hkl2\n",
    "            h_obj = HklPlane(h,k,l, lattice=lattice_material)\n",
    "            normal = np.round(h_obj.normal(), 6)\n",
    "\n",
    "            h_obj1 = HklPlane(h1,k1,l1, lattice=lattice_material)\n",
    "            family = h_obj1.get_family(h_obj1.miller_indices(), lattice=h_obj1._lattice, \n",
    "                                       include_friedel_pairs=True, crystal_structure=symmetry_)\n",
    "\n",
    "            normals = np.array([np.round(ijk.normal(),6) for ijk in family])\n",
    "\n",
    "            cond1 = h_obj in family\n",
    "            cond2 = np.any(np.all(normal == normals, axis=1))\n",
    "            return cond1 or cond2\n",
    "\n",
    "\n",
    "        ### verify prediction\n",
    "        import time\n",
    "        from fast_histogram import histogram1d\n",
    "        import random\n",
    "\n",
    "        length = []\n",
    "        nbgrains = 5\n",
    "        nbgrains1 = 5\n",
    "        material1_ = material_\n",
    "        total_prediction = 0\n",
    "        count_good = 0\n",
    "        count_bad = 0\n",
    "        for _ in trange(50):\n",
    "            noisy_data = bool(random.getrandbits(1)) \n",
    "            remove_peaks = bool(random.getrandbits(1)) \n",
    "            seednumber = np.random.randint(1e6)\n",
    "            tabledistancerandom, hkl_sol, \\\n",
    "                    s_posx, s_posy, s_I, s_tth, s_chi, g, g1  = prepare_LP_NB(nbgrains, nbgrains1,\n",
    "                                                                            material_, 0,\n",
    "                                                                            material1_ = material1_,\n",
    "                                                                            seed = seednumber,sortintensity=True,\n",
    "                                                                            detectorparameters=[79.553,979.32,932.31,0.37,0.447], \n",
    "                                                                            pixelsize=0.0734,\n",
    "                                                                            dim1=2018, dim2=2016, \n",
    "                                                                            emin=5, emax=23,\n",
    "                                                                            flag = 10, noisy_data=noisy_data,\n",
    "                                                                            remove_peaks = remove_peaks)\n",
    "            length.append(len(s_posx))\n",
    "\n",
    "            ## Not all spots needs to be recognized for now\n",
    "            spots_in_center = np.arange(0,len(s_tth))\n",
    "\n",
    "            start_time = time.time()\n",
    "            codebars_all = []\n",
    "            #print(\"Spots in center: \",len(spots_in_center))\n",
    "            for i in spots_in_center: ## identify the center HKL spots \n",
    "                spotangles = tabledistancerandom[i]\n",
    "                spotangles = np.delete(spotangles, i)# removing the self distance\n",
    "                # codebars = np.histogram(spotangles, bins=angbins)[0]\n",
    "                codebars = histogram1d(spotangles, range=[min(angbins),max(angbins)], bins=len(angbins)-1)\n",
    "                ## normalize the same way as training data\n",
    "                max_codebars = np.max(codebars)\n",
    "                codebars = codebars/ max_codebars\n",
    "                codebars_all.append(codebars)\n",
    "            ## reshape for the model to predict all spots at once\n",
    "            codebars = np.array(codebars_all)\n",
    "            ## Do prediction of all spots at once\n",
    "            prediction = model.predict(codebars)\n",
    "            max_pred = np.max(prediction, axis = 1)\n",
    "            class_predicted = np.argmax(prediction, axis = 1)\n",
    "            ## verify prediction (Assuming we dont know HKL solution)\n",
    "            for i, gg in enumerate(class_predicted):\n",
    "\n",
    "                if np.any(np.abs(hkl_sol[spots_in_center[i],:3]) > 5):\n",
    "                    continue\n",
    "\n",
    "                total_prediction += 1\n",
    "                isgoodprediction = commonclass(classhkl[gg], hkl_sol[spots_in_center[i],:3], lattice_material, symmetry)\n",
    "                if isgoodprediction:\n",
    "                    count_good += 1\n",
    "                else:\n",
    "                    #print(\"Predicted HKL: \", classhkl[gg], \" Actual HKL: \", hkl_sol[spots_in_center[i],:3])\n",
    "                    count_bad += 1\n",
    "            #print(\"Seed number for LP:\",seednumber)\n",
    "\n",
    "            end_time = time.time()\n",
    "        length = np.array(length)\n",
    "        print(\"****************************************************************************\")\n",
    "        print(\"Total spots attempted:\",total_prediction)\n",
    "        print(\"Total Good prediction:\",count_good)\n",
    "        print(\"Total Bad prediction:\",count_bad)\n",
    "        print(material_, np.average(length), np.std(length))\n",
    "        \n",
    "        text_file_logger.write(\"Total spots attempted:\" +str(total_prediction) + \"\\n\")\n",
    "        text_file_logger.write(\"Total Good prediction:\" +str(count_good) + \"\\n\")\n",
    "        text_file_logger.write(\"Total Bad prediction:\" +str(count_bad) + \"\\n\")\n",
    "        text_file_logger.write(material_ +\", \"+ str(np.average(length))+\", \"+ str(np.std(length)) + \"\\n\")\n",
    "        \n",
    "    text_file_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa37d55-849d-44f5-ac9f-50d18b83dc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91cdfe-7852-4b69-bfa8-b7e48b918fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a39a27-088d-4927-892e-0434088a2b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_10data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:14<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9187\n",
      "Total Good prediction: 34\n",
      "Total Bad prediction: 9153\n",
      "ZrO2 1963.64 109.0019742940466\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_50data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:38<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8526\n",
      "Total Good prediction: 4801\n",
      "Total Bad prediction: 3725\n",
      "ZrO2 1963.2 109.03834188027623\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_100data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:36<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8789\n",
      "Total Good prediction: 6428\n",
      "Total Bad prediction: 2361\n",
      "ZrO2 1966.08 114.0434724129356\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_150data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:28<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9138\n",
      "Total Good prediction: 7088\n",
      "Total Bad prediction: 2050\n",
      "ZrO2 1967.9 106.91141192594922\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_200data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:34<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9419\n",
      "Total Good prediction: 7516\n",
      "Total Bad prediction: 1903\n",
      "ZrO2 1932.94 103.78177296616202\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_250data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:34<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8885\n",
      "Total Good prediction: 7131\n",
      "Total Bad prediction: 1754\n",
      "ZrO2 1966.8 106.81198434632698\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_300data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:31<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8616\n",
      "Total Good prediction: 7176\n",
      "Total Bad prediction: 1440\n",
      "ZrO2 1961.48 102.92739965626257\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_350data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:37<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9286\n",
      "Total Good prediction: 7531\n",
      "Total Bad prediction: 1755\n",
      "ZrO2 1977.3 105.60421393107379\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_400data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:25<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9159\n",
      "Total Good prediction: 7445\n",
      "Total Bad prediction: 1714\n",
      "ZrO2 1932.14 108.7452086300817\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_450data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:15<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8489\n",
      "Total Good prediction: 7236\n",
      "Total Bad prediction: 1253\n",
      "ZrO2 1966.42 104.70589095175114\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_500data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:11<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9533\n",
      "Total Good prediction: 8188\n",
      "Total Bad prediction: 1345\n",
      "ZrO2 1983.86 97.82269879736502\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_550data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:17<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8592\n",
      "Total Good prediction: 7435\n",
      "Total Bad prediction: 1157\n",
      "ZrO2 1922.42 97.45031349359529\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_600data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:30<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8580\n",
      "Total Good prediction: 7314\n",
      "Total Bad prediction: 1266\n",
      "ZrO2 1958.02 107.47939151297795\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_700data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:23<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8158\n",
      "Total Good prediction: 7079\n",
      "Total Bad prediction: 1079\n",
      "ZrO2 1977.04 103.5969034286257\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_800data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:10<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8755\n",
      "Total Good prediction: 7619\n",
      "Total Bad prediction: 1136\n",
      "ZrO2 1947.36 105.1839835716446\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_900data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:20<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8984\n",
      "Total Good prediction: 7963\n",
      "Total Bad prediction: 1021\n",
      "ZrO2 1946.32 108.95456667804244\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_1000data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:24<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 9000\n",
      "Total Good prediction: 7976\n",
      "Total Bad prediction: 1024\n",
      "ZrO2 1971.84 103.66356351196886\n",
      "save directory is : C:\\Users\\purushot\\Anaconda3\\envs\\laueNN\\Lib\\site-packages\\lauetoolsnn\\example_notebook_scripts//ZrO2_2000data\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:18<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************\n",
      "Total spots attempted: 8965\n",
      "Total Good prediction: 8101\n",
      "Total Bad prediction: 864\n",
      "ZrO2 1958.44 102.21177231610847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "\n",
    "    ## if LaueToolsNN is properly installed\n",
    "    try:\n",
    "        from lauetoolsnn.utils_lauenn import generate_classHKL, generate_dataset, rmv_freq_class, get_material_detail, read_hdf5\n",
    "    except:\n",
    "        # else import from a path where LaueToolsNN files are\n",
    "        import sys\n",
    "        sys.path.append(r\"USER_PATH_HERE\")\n",
    "        from utils_lauenn import generate_classHKL, generate_dataset, rmv_freq_class,  get_material_detail, read_hdf5\n",
    "        \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import _pickle as cPickle\n",
    "    import itertools\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ## if LaueToolsNN is properly installed\n",
    "    try:\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "    except:\n",
    "        # else import from a path where LaueToolsNN files are\n",
    "        import sys\n",
    "        sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "        from utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from tqdm import trange\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    ## if LaueToolsNN is properly installed\n",
    "    try:\n",
    "        from lauetoolsnn.utils_lauenn import get_material_detail, prepare_LP_NB\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.lauetools import IOLaueTools as IOLT\n",
    "    except:\n",
    "        # else import from a path where LaueToolsNN files are\n",
    "        import sys\n",
    "        sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "        from utils_lauenn import get_material_detail, prepare_LP_NB\n",
    "        sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\lauetools\")\n",
    "        import dict_LaueTools as dictLT\n",
    "        import IOLaueTools as IOLT\n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    # =============================================================================\n",
    "    \n",
    "    text_file_logger = open(\"logger_ZrO2.txt\", \"w\")\n",
    "    bss = [1,5,10,15,20,20,20,20,20,50,50,50,50,50,50,50,50,50]\n",
    "    nmbr = [10,50,100,150,200,250,300,350,400,450,500,550,600,700,800,900,1000,2000]\n",
    "    for inums in range(len(nmbr)):\n",
    "        \n",
    "        epochs = 20\n",
    "        batch_size = bss[inums]\n",
    "        inum = nmbr[inums]\n",
    "\n",
    "        \n",
    "        input_params = {\n",
    "                        \"material_\": \"ZrO2\",             ## same key as used in dict_LaueTools\n",
    "                        \"material1_\": \"ZrO2\",            ## same key as used in dict_LaueTools\n",
    "                        \"prefix\" : \"data\",                 ## prefix for the folder to be created for training dataset\n",
    "                        \"symmetry\": \"monoclinic\",           ## crystal symmetry of material_\n",
    "                        \"symmetry1\": \"monoclinic\",          ## crystal symmetry of material1_\n",
    "                        \"SG\": 14,                     ## Space group of material_ (None if not known)\n",
    "                        \"SG1\": 14,                    ## Space group of material1_ (None if not known)\n",
    "                        \"hkl_max_identify\" : 5,        ## Maximum hkl index to classify in a Laue pattern\n",
    "                        \"maximum_angle_to_search\":90,  ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                        \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                        \"nb_grains_per_lp\" : 5,        ## max grains to be generated in a Laue Image\n",
    "                        \"grains_nb_simulate\" : inum,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                        ## Detector parameters (roughly) of the Experimental setup\n",
    "                        ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                        \"detectorparameters\" :  [79.553,979.32,932.31,0.37,0.447], \n",
    "                        \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                        \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                        \"dim2\":2016,\n",
    "                        \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                        \"emax\" : 22,\n",
    "                        }\n",
    "\n",
    "        input_params[\"prefix\"] = \"_\" + str(input_params[\"grains_nb_simulate\"]) + input_params[\"prefix\"]\n",
    "        text_file_logger.write(input_params[\"prefix\"] + \"\\n\")\n",
    "        material_= input_params[\"material_\"]\n",
    "        material1_= input_params[\"material1_\"]\n",
    "        n = input_params[\"hkl_max_identify\"]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        symm1_ = input_params[\"symmetry1\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        SG1 = input_params[\"SG1\"]\n",
    "\n",
    "        if material_ != material1_:\n",
    "            save_directory = os.getcwd()+\"//\"+material_+\"_\"+material1_+input_params[\"prefix\"]\n",
    "        else:\n",
    "            save_directory = os.getcwd()+\"//\"+material_+input_params[\"prefix\"]\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "            crystal, SG, rules1, symmetry1,\\\n",
    "            lattice_material1, crystal1, SG1 = get_material_detail(material_, SG, symm_,\n",
    "                                                                   material1_, SG1, symm1_)\n",
    "\n",
    "        ## End of data generation for Neural network training: all files are saved in the same folder to be later used for training and prediction\n",
    "        import numpy as np\n",
    "        import _pickle as cPickle\n",
    "        if material_ != material1_:\n",
    "            prefix1 = material_+\"_\"+material1_\n",
    "        else:\n",
    "            prefix1 = material_\n",
    "        ## load model related files and generate the model\n",
    "        json_file = open(save_directory+\"//model_\"+prefix1+\".json\", 'r')\n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        ind_mat = None\n",
    "        ind_mat1 = None  \n",
    "        load_weights = save_directory + \"//model_\"+prefix1+\".h5\"\n",
    "        wb = read_hdf5(load_weights)\n",
    "        temp_key = list(wb.keys())\n",
    "\n",
    "        # # load json and create model\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        print(\"Constructing model\")\n",
    "        model.load_weights(load_weights)\n",
    "        print(\"Uploading weights to model\")\n",
    "        print(\"All model files found and loaded\")\n",
    "\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.layers import BatchNormalization\n",
    "        import keras\n",
    "        from keras.regularizers import l2\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense, Activation, Dropout\n",
    "        from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "        from lattice import HklPlane\n",
    "        \n",
    "\n",
    "        def commonclass(hkl1, hkl2, lattice_material, symmetry_):\n",
    "            \"\"\" test if hkl1 and hkl2 belong to the same class\"\"\"\n",
    "            h,k,l = hkl1\n",
    "            h1,k1,l1 = hkl2\n",
    "            h_obj = HklPlane(h,k,l, lattice=lattice_material)\n",
    "            normal = np.round(h_obj.normal(), 6)\n",
    "\n",
    "            h_obj1 = HklPlane(h1,k1,l1, lattice=lattice_material)\n",
    "            family = h_obj1.get_family(h_obj1.miller_indices(), lattice=h_obj1._lattice, \n",
    "                                       include_friedel_pairs=True, crystal_structure=symmetry_)\n",
    "\n",
    "            normals = np.array([np.round(ijk.normal(),6) for ijk in family])\n",
    "\n",
    "            cond1 = h_obj in family\n",
    "            cond2 = np.any(np.all(normal == normals, axis=1))\n",
    "            return cond1 or cond2\n",
    "\n",
    "\n",
    "        ### verify prediction\n",
    "        import time\n",
    "        from fast_histogram import histogram1d\n",
    "        import random\n",
    "\n",
    "        length = []\n",
    "        nbgrains = 5\n",
    "        nbgrains1 = 5\n",
    "        material1_ = material_\n",
    "        total_prediction = 0\n",
    "        count_good = 0\n",
    "        count_bad = 0\n",
    "        for _ in trange(50):\n",
    "            noisy_data = bool(random.getrandbits(1)) \n",
    "            remove_peaks = bool(random.getrandbits(1)) \n",
    "            seednumber = np.random.randint(1e6)\n",
    "            tabledistancerandom, hkl_sol, \\\n",
    "                    s_posx, s_posy, s_I, s_tth, s_chi, g, g1  = prepare_LP_NB(nbgrains, nbgrains1,\n",
    "                                                                            material_, 0,\n",
    "                                                                            material1_ = material1_,\n",
    "                                                                            seed = seednumber,sortintensity=True,\n",
    "                                                                            detectorparameters=[79.553,979.32,932.31,0.37,0.447], \n",
    "                                                                            pixelsize=0.0734,\n",
    "                                                                            dim1=2018, dim2=2016, \n",
    "                                                                            emin=5, emax=23,\n",
    "                                                                            flag = 10, noisy_data=noisy_data,\n",
    "                                                                            remove_peaks = remove_peaks)\n",
    "            length.append(len(s_posx))\n",
    "\n",
    "            ## Not all spots needs to be recognized for now\n",
    "            spots_in_center = np.arange(0,len(s_tth))\n",
    "\n",
    "            start_time = time.time()\n",
    "            codebars_all = []\n",
    "            #print(\"Spots in center: \",len(spots_in_center))\n",
    "            for i in spots_in_center: ## identify the center HKL spots \n",
    "                spotangles = tabledistancerandom[i]\n",
    "                spotangles = np.delete(spotangles, i)# removing the self distance\n",
    "                # codebars = np.histogram(spotangles, bins=angbins)[0]\n",
    "                codebars = histogram1d(spotangles, range=[min(angbins),max(angbins)], bins=len(angbins)-1)\n",
    "                ## normalize the same way as training data\n",
    "                max_codebars = np.max(codebars)\n",
    "                codebars = codebars/ max_codebars\n",
    "                codebars_all.append(codebars)\n",
    "            ## reshape for the model to predict all spots at once\n",
    "            codebars = np.array(codebars_all)\n",
    "            ## Do prediction of all spots at once\n",
    "            prediction = model.predict(codebars)\n",
    "            max_pred = np.max(prediction, axis = 1)\n",
    "            class_predicted = np.argmax(prediction, axis = 1)\n",
    "            ## verify prediction (Assuming we dont know HKL solution)\n",
    "            for i, gg in enumerate(class_predicted):\n",
    "\n",
    "                if np.any(np.abs(hkl_sol[spots_in_center[i],:3]) > 5):\n",
    "                    continue\n",
    "\n",
    "                total_prediction += 1\n",
    "                isgoodprediction = commonclass(classhkl[gg], hkl_sol[spots_in_center[i],:3], lattice_material, symmetry)\n",
    "                if isgoodprediction:\n",
    "                    count_good += 1\n",
    "                else:\n",
    "                    #print(\"Predicted HKL: \", classhkl[gg], \" Actual HKL: \", hkl_sol[spots_in_center[i],:3])\n",
    "                    count_bad += 1\n",
    "            #print(\"Seed number for LP:\",seednumber)\n",
    "\n",
    "            end_time = time.time()\n",
    "        length = np.array(length)\n",
    "        print(\"****************************************************************************\")\n",
    "        print(\"Total spots attempted:\",total_prediction)\n",
    "        print(\"Total Good prediction:\",count_good)\n",
    "        print(\"Total Bad prediction:\",count_bad)\n",
    "        print(material_, np.average(length), np.std(length))\n",
    "        \n",
    "        text_file_logger.write(\"Total spots attempted:\" +str(total_prediction) + \"\\n\")\n",
    "        text_file_logger.write(\"Total Good prediction:\" +str(count_good) + \"\\n\")\n",
    "        text_file_logger.write(\"Total Bad prediction:\" +str(count_bad) + \"\\n\")\n",
    "        text_file_logger.write(material_ +\", \"+ str(np.average(length))+\", \"+ str(np.std(length)) + \"\\n\")\n",
    "        \n",
    "    text_file_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628b65a-37cf-40c3-b4b0-997380e9ed3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
