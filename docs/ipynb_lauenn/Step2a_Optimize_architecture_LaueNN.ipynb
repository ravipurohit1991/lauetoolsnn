{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Grid search optimization of hyperperameters of neural network architecture\n",
    "\n",
    "### Load the data generated in Step 1\n",
    "### Define the Neural network architecture (with hyper parameters to be optimized)\n",
    "### Run Sklearn GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules used for this Notebook\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## if LaueToolsNN is properly installed\n",
    "try:\n",
    "    from lauetoolsnn.utils_lauenn import vali_array\n",
    "except:\n",
    "    # else import from a path where LaueToolsNN files are\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "    from utils_lauenn import vali_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## step 1: define material and path to access the training dataset generated using Step 1 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory where training dataset is stored is : C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\example_notebook_scripts//Cu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "## User Input dictionary with parameters (reduced but same as the one used in STEP 1)\n",
    "## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "# =============================================================================\n",
    "input_params = {\n",
    "                \"material_\": \"Cu\",             ## same key as used in dict_LaueTools\n",
    "                \"material1_\": \"Cu\",            ## same key as used in dict_LaueTools\n",
    "                \"prefix\" : \"\",                 ## prefix for the folder to be created for training dataset\n",
    "                \"nb_grains_per_lp\" : 5,        ## max grains to be generated in a Laue Image\n",
    "                \"grains_nb_simulate\" : 100,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                \"batch_size\":50,               ## batches of files to use while training\n",
    "                \"epochs\":5,                    ## number of epochs for training\n",
    "                }\n",
    "\n",
    "material_= input_params[\"material_\"]\n",
    "material1_= input_params[\"material1_\"]\n",
    "nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "\n",
    "if material_ != material1_:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+\"_\"+material1_+input_params[\"prefix\"]\n",
    "else:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+input_params[\"prefix\"]\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    print(\"The directory doesn't exists; please veify the path\")\n",
    "else:\n",
    "    print(\"Directory where training dataset is stored is : \"+save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7b1f5-2ac1-45f6-8245-439ced9081d4",
   "metadata": {},
   "source": [
    "## Step 2: Load the necessary files generated in Step 1 script\n",
    "### Loading the Output class and ground truth; loading the training dataset of user defined batch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4155ee36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 13\n",
      "Number of spots in a batch of 20 files : 723\n",
      "Min, Max class ID is 0, 12\n"
     ]
    }
   ],
   "source": [
    "classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "n_bins = len(angbins)-1\n",
    "n_outputs = len(classhkl)\n",
    "print(n_bins, n_outputs)\n",
    "      \n",
    " ## Tuning on small dataset of Cu (20 corresponds to number of files)\n",
    "x_training, y_training = vali_array(save_directory+\"//training_data\", 20, \n",
    "                                    len(classhkl), \n",
    "                                    loc_new, print, tocategorical=False)\n",
    "print(\"Number of spots in a batch of %i files : %i\" %(20, len(x_training)))\n",
    "print(\"Min, Max class ID is %i, %i\" %(np.min(y_training), np.max(y_training)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be2bdc-587e-47d0-b4c1-9d32fe78afc2",
   "metadata": {},
   "source": [
    "## Step 3: Defining a neural network architecture with hyperparameters as free parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e577ef-7f75-4ef0-b873-57607e34e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "####################################################################################\n",
    "## General architecture with all free parameters\n",
    "####################################################################################\n",
    "def model_arch_general(kernel_coeff = 0.0005, \n",
    "                       bias_coeff = 0.0005, \n",
    "                       init_mode='uniform',\n",
    "                       learning_rate=0.0001, \n",
    "                       neurons_multiplier= [1200, 1200, (32*2*7)+(1200//2), (32*2*15), 32], \n",
    "                       layers=3, \n",
    "                       batch_norm=False, \n",
    "                       optimizer=\"adam\",\n",
    "                       activation='relu',\n",
    "                       dropout_rate=0.0, \n",
    "                       weight_constraint=0):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(keras.Input(shape=(int(neurons_multiplier[0]),)))\n",
    "    \n",
    "    if layers > 0:\n",
    "        for lay in range(layers):\n",
    "            ## Hidden layer n\n",
    "            if kernel_coeff == None and bias_coeff == None and\\\n",
    "                                weight_constraint == None and init_mode == None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]),))\n",
    "                \n",
    "            elif kernel_coeff == None and bias_coeff == None and\\\n",
    "                                weight_constraint == None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode))\n",
    "                \n",
    "            elif kernel_coeff == None and bias_coeff == None and\\\n",
    "                                init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_constraint=maxnorm(weight_constraint)))\n",
    "            \n",
    "            elif weight_constraint == None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff),))\n",
    "            \n",
    "            elif init_mode == None and weight_constraint == None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff),))\n",
    "                \n",
    "            elif kernel_coeff != None and bias_coeff != None and\\\n",
    "                                weight_constraint != None and init_mode != None:\n",
    "                model.add(Dense(int(neurons_multiplier[lay+1]), \n",
    "                                kernel_initializer=init_mode,\n",
    "                                kernel_regularizer=l2(kernel_coeff), \n",
    "                                bias_regularizer=l2(bias_coeff), \n",
    "                                kernel_constraint=maxnorm(weight_constraint)))\n",
    "            else:\n",
    "                print(\"condition not satisfied\")\n",
    "            \n",
    "            if batch_norm:\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    ## Output layer \n",
    "    model.add(Dense(int(neurons_multiplier[-1]), activation='softmax'))\n",
    "    if (optimizer == \"adam\" or optimizer == \"Adam\") and learning_rate != None:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "####################################################################################\n",
    "## General architecture with dropout as free parameters\n",
    "####################################################################################\n",
    "def model_arch_general_dropout(n_bins, n_outputs, activation = 'relu'):\n",
    "    \"\"\"\n",
    "    Very simple and straight forward Neural Network with few hyperparameters\n",
    "    straighforward RELU activation strategy with cross entropy to identify the HKL\n",
    "    Tried BatchNormalization --> no significant impact\n",
    "    Tried weighted approach --> not better for HCP\n",
    "    Trying Regularaization \n",
    "    l2(0.001) means that every coefficient in the weight matrix of the layer \n",
    "    will add 0.001 * weight_coefficient_value**2 to the total loss of the network\n",
    "    \"\"\"\n",
    "    if n_outputs >= n_bins:\n",
    "        param = n_bins\n",
    "        if param*15 < (2*n_outputs): ## quick hack; make Proper implementation\n",
    "            param = (n_bins + n_outputs)//2\n",
    "    else:\n",
    "        # param = n_outputs ## More reasonable ???\n",
    "        param = n_outputs*2 ## More reasonable ???\n",
    "        # param = n_bins//2\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(n_bins,)))\n",
    "    ## Hidden layer 1\n",
    "    model.add(Dense(n_bins, kernel_regularizer=l2(0.0005), bias_regularizer=l2( 0.0005)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.3)) ## Adding dropout as we introduce some uncertain data with noise\n",
    "    ## Hidden layer 2\n",
    "    model.add(Dense(((param)*15 + n_bins)//2, kernel_regularizer=l2( 0.0005), bias_regularizer=l2( 0.0005)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    ## Hidden layer 3\n",
    "    model.add(Dense((param)*15, kernel_regularizer=l2( 0.0005), bias_regularizer=l2( 0.0005)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    ## Output layer \n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    ## Compile model\n",
    "    otp = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=otp, metrics=[\"accuracy\"])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa48403-16da-4d41-8055-3f9b696bd4bc",
   "metadata": {},
   "source": [
    "## Step 4: Define hyperparameters and launch grid search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff9f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.875527 using {'activation': 'tanh', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.154885 (0.021484) with: {'activation': 'softmax', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.161829 (0.055889) with: {'activation': 'softplus', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.858975 (0.025399) with: {'activation': 'softsign', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.531197 (0.094889) with: {'activation': 'relu', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.875527 (0.025764) with: {'activation': 'tanh', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.077548 (0.045779) with: {'activation': 'sigmoid', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.117529 (0.029199) with: {'activation': 'hard_sigmoid', 'n_bins': 1200, 'n_outputs': 13}\n",
      "0.870000 (0.037068) with: {'activation': 'linear', 'n_bins': 1200, 'n_outputs': 13}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "if __name__ == '__main__':  # Multiprocessing enclosing\n",
    "    if material_ != material1_:\n",
    "        text_file = open(save_directory+\"//grid_optimizer_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "    else:\n",
    "        text_file = open(save_directory+\"//grid_optimizer_logger_\"+material_+\".txt\", \"w\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tuning neural network parameters\n",
    "    # Play with fixing and freeing different parameters\n",
    "    # =============================================================================\n",
    "\n",
    "    # Wrap Keras model so it can be used by scikit-learn\n",
    "    model_ann = KerasClassifier(build_fn=model_arch_general_dropout, verbose=0)\n",
    "    \n",
    "    # Create hyperparameter space\n",
    "    optimizer = ['adam']\n",
    "    learning_rate = [0.001] #[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    init_mode = [None] #['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "    dropout_rate = [0.3] #[0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    weight_constraint = [None] #[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    layers = [3]\n",
    "    kernel_coeff = [0.0005]\n",
    "    bias_coeff = [0.0005]\n",
    "    first_layer = int(n_outputs * 2)\n",
    "    neurons_multiplier = [[int(n_bins), int(n_bins), int(first_layer*7+n_bins/2), int(first_layer*15), int(n_outputs)],]\n",
    "    batch_norm = [False]\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    #hyperparameters = dict( \n",
    "    #                        kernel_coeff=kernel_coeff,\n",
    "    #                        bias_coeff=bias_coeff,\n",
    "    #                        init_mode=init_mode,\n",
    "    #                        learning_rate=learning_rate,\n",
    "    #                        neurons_multiplier=neurons_multiplier, \n",
    "    #                        layers=layers,\n",
    "    #                        batch_norm=batch_norm,\n",
    "    #                        optimizer=optimizer,\n",
    "    #                        activation=activation,\n",
    "    #                        dropout_rate=dropout_rate,\n",
    "    #                        weight_constraint=weight_constraint,\n",
    "    #                        )\n",
    "    hyperparameters = dict( \n",
    "                            n_bins=[n_bins],\n",
    "                            n_outputs=[n_outputs],\n",
    "                            activation=activation,\n",
    "                            )\n",
    "    \n",
    "    grid = GridSearchCV(estimator=model_ann, cv=5, param_grid=hyperparameters, n_jobs=-1)\n",
    "    # Fit grid search\n",
    "    grid_result = grid.fit(x_training, y_training)\n",
    "    # View hyperparameters of best neural network\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    text_file.write(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_) + \"\\n\")\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        text_file.write(\"%f (%f) with: %r\" % (mean, stdev, param) + \"\\n\")\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
