{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Training the neural network (supports single and two phase material)\n",
    "\n",
    "## Different steps of neural network training is outlined in this notebook (LaueToolsNN GUI does the same thing)\n",
    "\n",
    "### Load the data generated in Step 1\n",
    "### Define the Neural network architecture\n",
    "### Train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules used for this Notebook\n",
    "import numpy as np\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## if LaueToolsNN is properly installed\n",
    "try:\n",
    "    from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "except:\n",
    "    # else import from a path where LaueToolsNN files are\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\")\n",
    "    from utils_lauenn import array_generator, array_generator_verify, vali_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## step 1: define material and path to access the training dataset generated using Step 1 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory where training dataset is stored is : C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\example_notebook_scripts//Cu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "## User Input dictionary with parameters (reduced but same as the one used in STEP 1)\n",
    "## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "# =============================================================================\n",
    "input_params = {\n",
    "                \"material_\": \"Cu\",             ## same key as used in dict_LaueTools\n",
    "                \"material1_\": \"Cu\",            ## same key as used in dict_LaueTools\n",
    "                \"prefix\" : \"\",                 ## prefix for the folder to be created for training dataset\n",
    "                \"nb_grains_per_lp\" : 5,        ## max grains to be generated in a Laue Image\n",
    "                \"grains_nb_simulate\" : 500,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                \"batch_size\":50,               ## batches of files to use while training\n",
    "                \"epochs\":5,                    ## number of epochs for training\n",
    "                }\n",
    "\n",
    "material_= input_params[\"material_\"]\n",
    "material1_= input_params[\"material1_\"]\n",
    "nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "\n",
    "if material_ != material1_:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+\"_\"+material1_+input_params[\"prefix\"]\n",
    "else:\n",
    "    save_directory = os.getcwd()+\"//\"+material_+input_params[\"prefix\"]\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    print(\"The directory doesn't exists; please veify the path\")\n",
    "else:\n",
    "    print(\"Directory where training dataset is stored is : \"+save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7b1f5-2ac1-45f6-8245-439ced9081d4",
   "metadata": {},
   "source": [
    "## Step 2: Load the necessary files generated in Step 1 script\n",
    "### Loading the Output class and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4155ee36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 13\n"
     ]
    }
   ],
   "source": [
    "classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "    class_weights = cPickle.load(input_file)\n",
    "class_weights = class_weights[0]\n",
    "\n",
    "n_bins = len(angbins)-1\n",
    "n_outputs = len(classhkl)\n",
    "print(n_bins, n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be2bdc-587e-47d0-b4c1-9d32fe78afc2",
   "metadata": {},
   "source": [
    "## Step 3: Defining a neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e577ef-7f75-4ef0-b873-57607e34e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "metricsNN = [\n",
    "            keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "            keras.metrics.FalsePositives(name=\"fp\"),\n",
    "            keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "            keras.metrics.TruePositives(name=\"tp\"),\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"accuracy\"),\n",
    "            ]\n",
    "\n",
    "def model_arch_general_optimized(n_bins, n_outputs, kernel_coeff = 0.0005, bias_coeff = 0.0005, lr=None, verbose=1,\n",
    "                       write_to_console=None):\n",
    "    \"\"\"\n",
    "    Very simple and straight forward Neural Network with few hyperparameters\n",
    "    straighforward RELU activation strategy with cross entropy to identify the HKL\n",
    "    Tried BatchNormalization --> no significant impact\n",
    "    Tried weighted approach --> not better for HCP\n",
    "    Trying Regularaization \n",
    "    l2(0.001) means that every coefficient in the weight matrix of the layer \n",
    "    will add 0.001 * weight_coefficient_value**2 to the total loss of the network\n",
    "    1e-3,1e-5,1e-6\n",
    "    \"\"\"\n",
    "    if n_outputs >= n_bins:\n",
    "        param = n_bins\n",
    "        if param*15 < (2*n_outputs): ## quick hack; make Proper implementation\n",
    "            param = (n_bins + n_outputs)//2\n",
    "    else:\n",
    "        # param = n_outputs ## More reasonable ???\n",
    "        param = n_outputs*2 ## More reasonable ???\n",
    "        # param = n_bins//2\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(n_bins,)))\n",
    "    ## Hidden layer 1\n",
    "    model.add(Dense(n_bins, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3)) ## Adding dropout as we introduce some uncertain data with noise\n",
    "    ## Hidden layer 2\n",
    "    model.add(Dense(((param)*15 + n_bins)//2, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    ## Hidden layer 3\n",
    "    model.add(Dense((param)*15, kernel_regularizer=l2(kernel_coeff), bias_regularizer=l2(bias_coeff)))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    ## Output layer \n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    ## Compile model\n",
    "    if lr != None:\n",
    "        otp = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=otp, metrics=[metricsNN])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[metricsNN])\n",
    "    \n",
    "    if verbose == 1:\n",
    "        model.summary()\n",
    "        stringlist = []\n",
    "        model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        short_model_summary = \"\\n\".join(stringlist)\n",
    "        if write_to_console!=None:\n",
    "            write_to_console(short_model_summary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa48403-16da-4d41-8055-3f9b696bd4bc",
   "metadata": {},
   "source": [
    "## Step 4: Training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff9f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1200)              1441200   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1200)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1200)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 795)               954795    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 795)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 795)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 390)               310440    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 390)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 390)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 13)                5083      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,711,518\n",
      "Trainable params: 2,711,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of spots in a batch of 50 files : 1868\n",
      "Min, Max class ID is 0, 12\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 17s 314ms/step - loss: 0.8325 - fn: 22892.0000 - fp: 2514.0000 - tn: 1165746.0000 - tp: 74463.0000 - precision: 0.9673 - accuracy: 0.7649 - val_loss: 0.1034 - val_fn: 545.0000 - val_fp: 305.0000 - val_tn: 233251.0000 - val_tp: 18918.0000 - val_precision: 0.9841 - val_accuracy: 0.9720\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 12s 250ms/step - loss: 0.1323 - fn: 2741.0000 - fp: 1592.0000 - tn: 1166668.0000 - tp: 94614.0000 - precision: 0.9835 - accuracy: 0.9718 - val_loss: 0.0923 - val_fn: 441.0000 - val_fp: 289.0000 - val_tn: 233267.0000 - val_tp: 19022.0000 - val_precision: 0.9850 - val_accuracy: 0.9773\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 13s 264ms/step - loss: 0.0988 - fn: 1972.0000 - fp: 1267.0000 - tn: 1166993.0000 - tp: 95383.0000 - precision: 0.9869 - accuracy: 0.9797 - val_loss: 0.0914 - val_fn: 439.0000 - val_fp: 297.0000 - val_tn: 233259.0000 - val_tp: 19024.0000 - val_precision: 0.9846 - val_accuracy: 0.9774\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 0.0826 - fn: 1646.0000 - fp: 1067.0000 - tn: 1167193.0000 - tp: 95709.0000 - precision: 0.9890 - accuracy: 0.9831 - val_loss: 0.0890 - val_fn: 400.0000 - val_fp: 314.0000 - val_tn: 233242.0000 - val_tp: 19063.0000 - val_precision: 0.9838 - val_accuracy: 0.9794\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 13s 272ms/step - loss: 0.0726 - fn: 1353.0000 - fp: 918.0000 - tn: 1167342.0000 - tp: 96002.0000 - precision: 0.9905 - accuracy: 0.9861 - val_loss: 0.0898 - val_fn: 391.0000 - val_fp: 309.0000 - val_tn: 233247.0000 - val_tp: 19072.0000 - val_precision: 0.9841 - val_accuracy: 0.9799\n",
      "Saved model to disk\n",
      "Training Accuracy: 0.9861024022102356\n",
      "Training Loss: 0.07262731343507767\n",
      "Validation Accuracy: 0.9799106121063232\n",
      "Validation Loss: 0.08980165421962738\n"
     ]
    }
   ],
   "source": [
    "# load model and train\n",
    "#neurons_multiplier is a list with number of neurons per layer, the first value is input shape and last value is output shape, inbetween are the number of neurons per hidden layers\n",
    "model = model_arch_general_optimized(  n_bins, n_outputs,\n",
    "                                       kernel_coeff = 1e-5,\n",
    "                                       bias_coeff = 1e-6,\n",
    "                                       lr = 1e-3,\n",
    "                                        )\n",
    "#model = model_arch_general_optimized(n_bins, n_outputs, kernel_coeff = 0.0005, bias_coeff = 0.0005, lr=None, verbose=1,)\n",
    "## temp function to quantify the spots and classes present in a batch\n",
    "batch_size = input_params[\"batch_size\"] \n",
    "trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", batch_size, \n",
    "                                        len(classhkl), loc_new, print)\n",
    "print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "\n",
    "epochs = input_params[\"epochs\"] \n",
    "\n",
    "## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "nb_grains_per_lp1 = nb_grains_per_lp\n",
    "if material_ != material1_:\n",
    "    nb_grains_list = list(range(nb_grains_per_lp+1))\n",
    "    nb_grains1_list = list(range(nb_grains_per_lp1+1))\n",
    "    list_permute = list(itertools.product(nb_grains_list, nb_grains1_list))\n",
    "    list_permute.pop(0)\n",
    "    steps_per_epoch = (len(list_permute) * grains_nb_simulate)//batch_size\n",
    "else:\n",
    "    steps_per_epoch = int((nb_grains_per_lp * grains_nb_simulate) / batch_size)\n",
    "\n",
    "val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1\n",
    "if val_steps_per_epoch == 0:\n",
    "    val_steps_per_epoch = 1 \n",
    "    \n",
    "## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "training_data_generator = array_generator(save_directory+\"//training_data\", batch_size, \\\n",
    "                                          len(classhkl), loc_new, print)\n",
    "testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size, \\\n",
    "                                          len(classhkl), loc_new, print)\n",
    "\n",
    "######### TRAIN THE DATA\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", monitor='val_accuracy', \n",
    "                      mode='max', save_best_only=True)\n",
    "\n",
    "# model save directory and filename\n",
    "if material_ != material1_:\n",
    "    model_name = save_directory+\"//model_\"+material_+\"_\"+material1_\n",
    "else:\n",
    "    model_name = save_directory+\"//model_\"+material_\n",
    "\n",
    "## Fitting function\n",
    "stats_model = model.fit(\n",
    "                        training_data_generator, \n",
    "                        epochs=epochs, \n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=testing_data_generator,\n",
    "                        validation_steps=val_steps_per_epoch,\n",
    "                        verbose=1,\n",
    "                        class_weight=class_weights,\n",
    "                        callbacks=[es, ms]\n",
    "                        )\n",
    "\n",
    "# Save model config and weights\n",
    "model_json = model.to_json()\n",
    "with open(model_name+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)            \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_name+\".h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "\n",
    "# Plot the accuracy/loss v Epochs\n",
    "epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "ax[1].legend()\n",
    "if material_ != material1_:\n",
    "    plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\"_\"+material1_+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "else:\n",
    "    plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "plt.close()\n",
    "\n",
    "if material_ != material1_:\n",
    "    text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "else:\n",
    "    text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\".txt\", \"w\")\n",
    "\n",
    "text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "for inj in range(len(epochs)):\n",
    "    string1 = str(epochs[inj]) + \",\"+ str(model.history.history['loss'][inj])+\\\n",
    "            \",\"+str(model.history.history['val_loss'][inj])+\",\"+str(model.history.history['accuracy'][inj])+\\\n",
    "            \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "    text_file.write(string1)\n",
    "text_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f1b59-830b-42d3-9c20-74663c27b136",
   "metadata": {},
   "source": [
    "## Stats on the trained model with sklearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bc9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97        61\n",
      "           1       0.94      1.00      0.97        49\n",
      "           2       1.00      1.00      1.00        87\n",
      "           3       1.00      0.96      0.98       182\n",
      "           4       0.97      0.98      0.98       178\n",
      "           5       0.99      0.99      0.99       183\n",
      "           6       0.98      0.99      0.99       177\n",
      "           7       0.95      0.96      0.96       169\n",
      "           8       0.98      0.98      0.98       180\n",
      "           9       0.98      0.98      0.98       338\n",
      "          10       0.99      0.96      0.97       146\n",
      "          11       0.97      0.97      0.97       138\n",
      "          12       0.97      0.97      0.97       114\n",
      "\n",
      "    accuracy                           0.98      2002\n",
      "   macro avg       0.98      0.98      0.98      2002\n",
      "weighted avg       0.98      0.98      0.98      2002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## verify the \n",
    "x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
